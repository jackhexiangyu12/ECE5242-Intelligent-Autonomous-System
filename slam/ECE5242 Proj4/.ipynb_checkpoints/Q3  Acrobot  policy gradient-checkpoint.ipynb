{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Environment and parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped\n",
    "# Policy gradient has high variance, seed for reproducability\n",
    "env.seed(1)\n",
    "## ENVIRONMENT Hyperparameters\n",
    "num_state=env.observation_space.shape[0]\n",
    "num_action=env.action_space.n\n",
    "\n",
    "## TRAINING Hyperparameters\n",
    "max_episodes = 300\n",
    "learning_rate = 0.01\n",
    "discount=0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_episode_reward_func(episode_reward):\n",
    "    discounted_episode_reward= np.zeros_like(episode_reward)\n",
    "    cumulative=0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative=cumulative*discount+episode_rewards[i]\n",
    "        discounted_episode_rewards[i]=cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_reward= (discounted_episode_reward- mean) / (std)\n",
    "    \n",
    "    return discounted_episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Policy gradient neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'actions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-33b554092783>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# If you have single-class labels, where an object can only belong to one class, you might now consider using\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mneg_log_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfc3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_log_prob\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdiscounted_episode_rewards_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'actions' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"inputs\"):\n",
    "    \n",
    "    #input\n",
    "    state_ = tf.placeholder(tf.float32, [None, state_size], name=\"state_\")\n",
    "    action_= tf.placeholder(tf.int32, [None, action_size], name=\"action_\")\n",
    "    discounted_episode_rewards_= tf.placeholder(tf.float32, [None,], name=\"discounted_episode_rewards_\")\n",
    "    \n",
    "    \n",
    "    with tf.name_scope(\"fc1\"):\n",
    "        fc1 = tf.contrib.layers.fully_connected(inputs =state_,\n",
    "                                                num_outputs=10,\n",
    "                                                activation_fn=tf.nn.relu,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    with tf.name_scope(\"fc2\"):\n",
    "        fc2 = tf.contrib.layers.fully_connected(inputs = fc1,\n",
    "                                                num_outputs=num_action,\n",
    "                                                activation_fn= tf.nn.relu,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    with tf.name_scope(\"fc3\"):\n",
    "        fc3 = tf.contrib.layers.fully_connected(inputs = fc2,\n",
    "                                                num_outputs=num_action,\n",
    "                                                activation_fn= None,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    with tf.name_scope(\"softmax\"):\n",
    "        action_distribution = tf.nn.softmax(fc3)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = fc3,labels=action_)\n",
    "        loss = tf.reduce_mean(neg_log_prob * discounted_episode_reward_) \n",
    "        \n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_opt=tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allRewards=[]\n",
    "total_rewards=0\n",
    "maximumRewardRecorded=0\n",
    "episode=0\n",
    "state_list,action_list,reward_list=[],[],[]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(max_episodes): #per episode\n",
    "        episode_rewards_sum = 0\n",
    "        state = env.reset()\n",
    "        if i>=(episodes - 20):\n",
    "                env.render()\n",
    "                \n",
    "        while done!=True:\n",
    "            action_dis= sess.run(action_dis, feed_dict={x: state.reshape([1,4])}) #action from NN\n",
    "            action = np.random.choice(range(action_dis.shape[1]), p=action_dis.ravel())\n",
    "\n",
    "            new_state, reward, done, info = env.step(action)#trans2 new state\n",
    "            cc=np.zeros(num_action)\n",
    "            cc[action]=1\n",
    "            \n",
    "            episode_state.append(state)\n",
    "            episode_action.append(cc)\n",
    "            episode_reward.append(reward)\n",
    "            \n",
    "            state=new_state\n",
    "         \n",
    "        #per episode\n",
    "        episode_reward_list.append(np.sum(episode_reward))\n",
    "        discounted_episode_reward=discounted_episode_reward_func(episode_reward)\n",
    "        \n",
    "        # NN\n",
    "        loss_, _ = sess.run([loss, train_opt], feed_dict={state_: np.vstack(np.array(episode_state)),\n",
    "                                                          action_: np.vstack(np.array(episode_action)),\n",
    "                                                          discounted_episode_reward_: discounted_episode_reward \n",
    "                                                                })\n",
    "                \n",
    "        #reset\n",
    "        episode_state, episode_action, episode_reward= [],[],[]\n",
    "        \n",
    "        #average reward\n",
    "        if (i+1) % 100 == 0:\n",
    "            ave_reward=np.mean(reward_episode_list)\n",
    "            ave_reward_list.append(ave_reward)\n",
    "            reward_episode_list=[]\n",
    "            print('Episode {} Average Reward:{}'.format(i+1, ave_reward))\n",
    "                   \n",
    "        # save model\n",
    "        #if episode % 100 == 0:\n",
    "        #    saver.save(sess, \"./models/model.ckpt\")\n",
    "        #    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average reward\n",
    "plt.plot(100*(np.arange(len(ave_reward_list)) + 1), ave_reward_list)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.savefig('Acrobot.jpg')     \n",
    "plt.close()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

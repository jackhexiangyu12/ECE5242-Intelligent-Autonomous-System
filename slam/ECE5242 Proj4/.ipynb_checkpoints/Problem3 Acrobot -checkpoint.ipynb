{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import sys\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.kernel_approximation import RBFSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('rbf1', RBFSampler(gamma=5.0, n_components=100, random_state=None)), ('rbf2', RBFSampler(gamma=2.0, n_components=100, random_state=None)), ('rbf3', RBFSampler(gamma=1.0, n_components=100, random_state=None)), ('rbf4', RBFSampler(gamma=0.5, n_components=100, random_state=None))],\n",
       "       transformer_weights=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approximate.fit(scaler.transform(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 100\n",
    "epsilon = 0.0\n",
    "discount_factor = 0.99 \n",
    "epsilon = 0.1 \n",
    "epsilon_decay = 1.0\n",
    "\n",
    "#####################################################\n",
    "\n",
    "if mc:\n",
    "\tenv = gym.envs.make(\"MountainCar-v0\")\n",
    "else:\n",
    "\tenv = gym.envs.make(\"Acrobot-v1\")\n",
    "\n",
    "itersGraph = namedtuple(\"Graphs\",[\"iter_lengths\", \"iter_rewards\"])\n",
    "\n",
    "# Take samples as inputs for RBF kernels approximation of states\n",
    "samples = np.array([env.observation_space.sample() for x in range(10000)])\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(samples)\n",
    "\n",
    "approximate = sklearn.pipeline.FeatureUnion([(\n",
    "        \"rbf1\", RBFSampler(gamma=5.0, n_components=100)),\n",
    "        (\"rbf2\", RBFSampler(gamma=2.0, n_components=100)),\n",
    "        (\"rbf3\", RBFSampler(gamma=1.0, n_components=100)),\n",
    "        (\"rbf4\", RBFSampler(gamma=0.5, n_components=100))])\n",
    "approximate.fit(scaler.transform(samples))\n",
    "\n",
    "# Featurizes states over different RBF kernels\n",
    "def predict(s, a=None):\n",
    "\tscaled = scaler.transform([s])\n",
    "\tfeatures = approximate.transform(scaled)[0]\n",
    "\tif not a:\n",
    "\t\treturn np.array([m.predict([features])[0] for m in models])\n",
    "\telse:\n",
    "\t\treturn models[a].predict([features])[0]\n",
    "\n",
    "def epsilonGreedyPolicy(epsilon, nA):\n",
    "\tdef policy_fn(observation):\n",
    "\t\tA = np.ones(nA, dtype=float) * epsilon / nA\n",
    "\t\tq_values = predict(observation)\n",
    "\t\tbest_action = np.argmax(q_values)\n",
    "\t\tA[best_action] += (1.0 - epsilon)\n",
    "\t\treturn A\n",
    "\treturn policy_fn\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "count=0\n",
    "for _ in range(env.action_space.n):\n",
    "    model = SGDRegressor(learning_rate=\"constant\")\n",
    "    model.partial_fit([approximate.transform(scaler.transform([env.reset()]))[0]], [0])\n",
    "    models.append(model)\n",
    "    count+=1\n",
    "\n",
    "graph = itersGraph(iter_lengths=np.zeros(iters),iter_rewards=np.zeros(iters))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09317615, -0.06236118, -0.06751522, -0.13473764, -0.10436174,\n",
       "       -0.11979075, -0.1151801 , -0.05810292, -0.10191676, -0.12805527,\n",
       "        0.13950975, -0.06197658,  0.04490839, -0.12668634, -0.07827094,\n",
       "        0.04170145,  0.10309069,  0.11638538, -0.07372585, -0.01790838,\n",
       "       -0.00210292,  0.08660647, -0.13928258,  0.04552382, -0.12700477,\n",
       "       -0.01495117, -0.14041357,  0.01850375, -0.12839064, -0.1381517 ,\n",
       "       -0.07778379,  0.12863123, -0.10182033,  0.1240882 ,  0.14059795,\n",
       "        0.05891447,  0.0535927 ,  0.0999759 , -0.053369  , -0.12815251,\n",
       "        0.07504712, -0.13837025, -0.07961699,  0.02208811, -0.11819522,\n",
       "       -0.0539306 ,  0.12231373,  0.11923215, -0.00632304,  0.10846049,\n",
       "        0.04960405, -0.14142087, -0.01897091,  0.03672174,  0.00174729,\n",
       "        0.08432265, -0.12210947, -0.10508561,  0.01448842,  0.11337608,\n",
       "       -0.14072002, -0.13996518,  0.08119319, -0.10198644, -0.1360488 ,\n",
       "       -0.08012341,  0.08699688,  0.00135756,  0.02470901, -0.14037723,\n",
       "        0.03110677,  0.14003164, -0.06369868,  0.09868611,  0.07927704,\n",
       "       -0.13731636,  0.09459079,  0.13444633,  0.06531539,  0.08605575,\n",
       "        0.11506502, -0.08218496, -0.04071902, -0.10883051,  0.04337634,\n",
       "        0.13809808,  0.01793524, -0.0161999 , -0.00861243, -0.00627955,\n",
       "       -0.09530753, -0.07970907,  0.04080892, -0.13340219,  0.02889544,\n",
       "        0.141168  , -0.04391142,  0.08807144,  0.11509179, -0.13662718])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q-learning\n",
    "for i in range(iters):\n",
    "\n",
    "    policy = epsilonGreedyPolicy(epsilon * epsilon_decay**i, env.action_space.n)\n",
    "    last_reward = graph.iter_rewards[i - 1]\n",
    "    sys.stdout.flush()\n",
    "    state = env.reset()\n",
    "    next_action = None\n",
    "\n",
    "    for t in itertools.count(): #each iteration\n",
    "        if next_action is None:\n",
    "            action_probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        else:\n",
    "            action = next_action\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        graph.iter_rewards[i] += reward\n",
    "        graph.iter_lengths[i] = t\n",
    "\n",
    "        a = None \n",
    "        if not a:\n",
    "            Qvalnext = np.array([m.predict([approximate.transform(scaler.transform([next_state]))[0]])[0] for m in models])\n",
    "        else:\n",
    "            Qvalnext = models[a].predict([approximate.transform(scaler.transform([next_state]))[0]])[0]\n",
    "\n",
    "        td_target = reward + discount_factor * np.max(Qvalnext)\n",
    "        models[action].partial_fit([approximate.transform(scaler.transform([state]))[0]], [td_target])\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "\n",
    "    print \"Episode {0}/{1} (Last Reward = {2})\".format(i + 1, iters, last_reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(10,5))\n",
    "plt.plot(graph.iter_lengths)\n",
    "plt.xlabel(\"#Iterations\")\n",
    "plt.ylabel(\"Iteration Length\")\n",
    "plt.savefig('Results/'+st+\"_iteration_length.png\")\n",
    "plt.show(fig1)\n",
    "\n",
    "fig2 = plt.figure(figsize=(10,5))\n",
    "rewards_smoothed = pd.Series(graph.iter_rewards).rolling(25, min_periods=25).mean()\n",
    "plt.plot(rewards_smoothed)\n",
    "plt.xlabel(\"#Iterations\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.savefig('Results/'+st+\"_rewards.png\")\n",
    "plt.show(fig2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n",
    "y = [0, 0, 1, 1]\n",
    "rbf_feature = RBFSampler(gamma=1, random_state=1)\n",
    "X_features = rbf_feature.fit_transform(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

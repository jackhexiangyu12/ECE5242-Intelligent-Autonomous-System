{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In MountainCar, we discretize continuous state space and run Q learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "env = gym.make('MountainCar-v0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Function Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_state(state):\n",
    "    cc = (state - env.observation_space.low)*np.array([10, 100])\n",
    "    output= np.round(cc, 0).astype(int)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning(env, learning_rate, discount, epsilon, min_epsilon, episodes,num_state):\n",
    "    #epsilon:select action\n",
    "    #discount:update Q table\n",
    "    #episodes:iterations\n",
    "    \n",
    "    # Initialize\n",
    "    Q = np.random.uniform(low = -1,high = 1,size = (num_state[0], num_state[1],env.action_space.n))\n",
    "    reward_list = []\n",
    "    ave_reward_list = []\n",
    "    \n",
    "    # epsilon reduction for each iteration\n",
    "    reduction = (epsilon - min_epsilon)/episodes\n",
    "    \n",
    "    #iteration\n",
    "    for i in range(episodes):\n",
    "        \n",
    "        reward_periteration=0\n",
    "        currstate= env.reset()\n",
    "        currstate_dis=discretize_state(currstate)\n",
    "        done = False\n",
    "        \n",
    "        while done != True:   \n",
    "            \n",
    "            if i>=(episodes - 20):\n",
    "                env.render()\n",
    "            if np.random.random() < 1 - epsilon: # select action,epsilon-greedy algo\n",
    "                action = np.argmax(Q[currstate_dis[0], currstate_dis[1]]) \n",
    "            else:\n",
    "                action = np.random.randint(0, env.action_space.n)\n",
    "                \n",
    "            nextstate,reward,done,other= env.step(action) #next state and reward\n",
    "            nextstate_dis=discretize_state(nextstate)\n",
    "            \n",
    "            if done and nextstate[0] >= 0.5: #update Q table\n",
    "                Q[currstate_dis[0], currstate_dis[1], action] = reward\n",
    "            else:\n",
    "                gd=learning_rate*(reward + discount*np.max(Q[nextstate_dis[0],nextstate_dis[1]]) - Q[currstate_dis[0], currstate_dis[1],action])\n",
    "                Q[currstate_dis[0], currstate_dis[1],action] +=gd\n",
    "                                     \n",
    "            reward_periteration+=reward\n",
    "            currstate_dis = nextstate_dis\n",
    "        \n",
    "        # update epsilon\n",
    "        if epsilon > min_epsilon:\n",
    "            epsilon-=reduction\n",
    "        \n",
    "        reward_list.append(reward_periteration)\n",
    "        \n",
    "        #average reward\n",
    "        if (i+1) % 100 == 0:\n",
    "            ave_reward=np.mean(reward_list)\n",
    "            ave_reward_list.append(ave_reward)\n",
    "            reward_list = []\n",
    "            print('Episode {} Average Reward: {}'.format(i+1, ave_reward))\n",
    "            \n",
    "            \n",
    "            \n",
    "    env.close()\n",
    "    \n",
    "    return ave_reward_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run Qlearning and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 Average Reward: -200.0\n",
      "Episode 200 Average Reward: -200.0\n",
      "Episode 300 Average Reward: -200.0\n",
      "Episode 400 Average Reward: -200.0\n",
      "Episode 500 Average Reward: -200.0\n",
      "Episode 600 Average Reward: -200.0\n",
      "Episode 700 Average Reward: -200.0\n",
      "Episode 800 Average Reward: -200.0\n",
      "Episode 900 Average Reward: -200.0\n",
      "Episode 1000 Average Reward: -200.0\n",
      "Episode 1100 Average Reward: -200.0\n",
      "Episode 1200 Average Reward: -200.0\n",
      "Episode 1300 Average Reward: -200.0\n",
      "Episode 1400 Average Reward: -200.0\n",
      "Episode 1500 Average Reward: -200.0\n",
      "Episode 1600 Average Reward: -200.0\n",
      "Episode 1700 Average Reward: -200.0\n",
      "Episode 1800 Average Reward: -200.0\n",
      "Episode 1900 Average Reward: -200.0\n",
      "Episode 2000 Average Reward: -200.0\n",
      "Episode 2100 Average Reward: -200.0\n",
      "Episode 2200 Average Reward: -200.0\n",
      "Episode 2300 Average Reward: -200.0\n",
      "Episode 2400 Average Reward: -200.0\n",
      "Episode 2500 Average Reward: -200.0\n",
      "Episode 2600 Average Reward: -200.0\n",
      "Episode 2700 Average Reward: -199.7\n",
      "Episode 2800 Average Reward: -200.0\n",
      "Episode 2900 Average Reward: -200.0\n",
      "Episode 3000 Average Reward: -199.6\n",
      "Episode 3100 Average Reward: -200.0\n",
      "Episode 3200 Average Reward: -200.0\n",
      "Episode 3300 Average Reward: -200.0\n",
      "Episode 3400 Average Reward: -199.61\n",
      "Episode 3500 Average Reward: -199.09\n",
      "Episode 3600 Average Reward: -199.27\n",
      "Episode 3700 Average Reward: -198.56\n",
      "Episode 3800 Average Reward: -198.48\n",
      "Episode 3900 Average Reward: -197.58\n",
      "Episode 4000 Average Reward: -199.86\n",
      "Episode 4100 Average Reward: -199.97\n",
      "Episode 4200 Average Reward: -197.13\n",
      "Episode 4300 Average Reward: -196.36\n",
      "Episode 4400 Average Reward: -196.52\n",
      "Episode 4500 Average Reward: -197.61\n",
      "Episode 4600 Average Reward: -196.19\n",
      "Episode 4700 Average Reward: -182.64\n",
      "Episode 4800 Average Reward: -179.28\n",
      "Episode 4900 Average Reward: -181.26\n",
      "Episode 5000 Average Reward: -177.26\n"
     ]
    }
   ],
   "source": [
    "num_state= (env.observation_space.high - env.observation_space.low)*np.array([10, 100])\n",
    "num_state= np.round(num_state, 0).astype(int) + 1\n",
    "rewards = QLearning(env, 0.2, 0.9, 0.8, 0, 5000,num_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average reward\n",
    "plt.plot(100*(np.arange(len(rewards)) + 1), rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward vs Episodes')\n",
    "plt.savefig('MoutainCar_rewards.jpg')     \n",
    "plt.close()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
